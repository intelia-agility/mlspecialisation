{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56243a96-986d-46b8-97b5-64632a7267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --user \"kfp\" google-cloud-aiplatform google-cloud-storage category_encoders pandas google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c01a94f-f9a1-41b9-814a-de2b3bf38bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"blackfridayintelia\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = \"gs://blackfriday_kfp_pipeline\"\n",
    "DATA_URI = \"gs://blackfriday_data/train.csv\"\n",
    "SERVICE_ACCOUNT = \"397218668039-compute@developer.gserviceaccount.com\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/xgb\".format(BUCKET_URI)\n",
    "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/xgboost-cpu.1-1:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4775b60c-90cf-40c4-a394-96452d2a8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $DATA_URI\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $DATA_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462e0f8a-13e7-4aaa-877c-537e8805a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple\n",
    "import google.cloud.aiplatform as aip\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "# from kfp.v2.components import importer_node\n",
    "from kfp.dsl import Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component, Metrics\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                              ModelDeployOp)\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3382a4a2-b70b-4c9a-bf52-7a48d24bc8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b24bec-541a-44ef-ad8d-baf1ae8ce9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.9', \n",
    "           packages_to_install=['pandas', 'pyarrow'])\n",
    "def dataloader(\n",
    "    # An input parameter of type string.\n",
    "    message: str,\n",
    "    # input_dataset\n",
    "    imported_dataset: Input[Dataset],\n",
    "    # Use Output to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(imported_dataset.path)\n",
    "    \n",
    "    df['User_ID'] = df['User_ID'].astype(str)\n",
    "    df['Occupation'] = df['Occupation'].astype(str)\n",
    "    df['Marital_Status'] = df['Marital_Status'].astype(str)\n",
    "    df['Product_Category_1'] = df['Product_Category_1'].astype(str)\n",
    "    df['Product_Category_2'] = df['Product_Category_2'].astype(str)\n",
    "    df['Product_Category_3'] = df['Product_Category_3'].astype(str)\n",
    "    \n",
    "    print(\"description:\")\n",
    "    print(df['Purchase'].describe())\n",
    "    \n",
    "    output_dataset.metadata[\"message\"] = message\n",
    "    output_dataset.metadata[\"shape\"] = df.shape\n",
    "    output_dataset.metadata[\"format\"] = \"parquet\"\n",
    "    \n",
    "    df.to_parquet(output_dataset.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d909b16c-3012-433d-880f-4ace5a1b315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.9', \n",
    "           packages_to_install=['pandas', 'pyarrow'])\n",
    "def transformation(\n",
    "    # input_dataset\n",
    "    imported_dataset: Input[Dataset],\n",
    "    # Use Output to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    df = pd.read_parquet(imported_dataset.path)\n",
    "    \n",
    "    print('data scaling by sqrt and shrink into [0, 10]')\n",
    "    df['Purchase'] = (df['Purchase'].pow(1/2)-3.464)/15\n",
    "    \n",
    "    print(\"description:\")\n",
    "    print(df['Purchase'].describe())\n",
    "    \n",
    "    output_dataset.metadata[\"message\"] = \"Feature 'Purchase' sqrt transformed and shrinked into [0.0, 10.0]\"\n",
    "    output_dataset.metadata[\"shape\"] = df.shape\n",
    "    output_dataset.metadata[\"format\"] = \"parquet\"\n",
    "    \n",
    "    df.to_parquet(output_dataset.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed6eeb1-2e66-4f0e-9212-6bc136c17777",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.9', \n",
    "           packages_to_install=['pandas', 'pyarrow', 'scikit-learn'])\n",
    "def traintestsplit(\n",
    "    # input_dataset\n",
    "    imported_dataset: Input[Dataset],\n",
    "    random_seed: int,\n",
    "    test_size: float,\n",
    "    # Use Output to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    X_train_dataset: Output[Dataset],\n",
    "    y_train_dataset: Output[Dataset],\n",
    "    X_test_dataset: Output[Dataset],\n",
    "    y_test_dataset: Output[Dataset]    \n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    \n",
    "    df = pd.read_parquet(imported_dataset.path)\n",
    "    \n",
    "    print('train test split')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Purchase', axis=1), df['Purchase'], random_state=random_seed, test_size=test_size)\n",
    "\n",
    "    odd_users = X_test[~X_test.User_ID.isin( X_train.User_ID.unique())].shape[0]\n",
    "    \n",
    "    X_train_dataset.metadata[\"message\"] = \"X_train_dataset\"\n",
    "    X_train_dataset.metadata[\"shape\"] = X_train.shape\n",
    "    X_train_dataset.metadata[\"format\"] = \"parquet\"\n",
    "    X_train.to_parquet(X_train_dataset.path)\n",
    "    \n",
    "    X_test_dataset.metadata[\"message\"] = \"X_test_dataset\"\n",
    "    X_test_dataset.metadata[\"shape\"] = X_test.shape\n",
    "    X_test_dataset.metadata[\"format\"] = \"parquet\"\n",
    "    X_test_dataset.metadata[\"odd_users\"] = odd_users\n",
    "    X_test.to_parquet(X_test_dataset.path)\n",
    "    \n",
    "    y_train_dataset.metadata[\"message\"] = \"y_train_dataset\"\n",
    "    y_train_dataset.metadata[\"shape\"] = y_train.shape\n",
    "    y_train_dataset.metadata[\"format\"] = \"pickle\"\n",
    "    with open(y_train_dataset.path, 'wb') as f:\n",
    "        pickle.dump(y_train, f)\n",
    "    \n",
    "    y_test_dataset.metadata[\"message\"] = \"y_test_dataset\"\n",
    "    y_test_dataset.metadata[\"shape\"] = y_test.shape\n",
    "    y_test_dataset.metadata[\"format\"] = \"pickle\"\n",
    "    y_test_dataset.metadata[\"odd_users\"] = odd_users\n",
    "    with open(y_test_dataset.path, 'wb') as f:\n",
    "        pickle.dump(y_test, f)\n",
    "        \n",
    "    print(min(y_train), max(y_train))\n",
    "    print(min(y_test), max(y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8522b0-635e-41c1-aa7d-fc5edd9dd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.9', \n",
    "           packages_to_install=['pandas', 'pyarrow', 'category_encoders'])\n",
    "def target_encoding(\n",
    "    # input_dataset\n",
    "    imported_X_train: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_y_train: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_X_test: Input[Dataset],\n",
    "    # Use Output to get a metadata-rich handle to the output artifact\n",
    "    # of type `Dataset`.\n",
    "    output_X_train: Output[Dataset],\n",
    "    # of type `Dataset`.\n",
    "    output_X_test: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from category_encoders import TargetEncoder\n",
    "    import pickle\n",
    "    \n",
    "    X_train = pd.read_parquet(imported_X_train.path)\n",
    "    X_test = pd.read_parquet(imported_X_test.path)\n",
    "\n",
    "    with open(imported_y_train.path, 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "\n",
    "    print('target encode all the categorical features')\n",
    "    encoder = TargetEncoder()\n",
    "    enc = encoder.fit(X=X_train, y=y_train)\n",
    "\n",
    "    df_train_X = enc.transform(X_train)\n",
    "    df_test_X = enc.transform(X_test)\n",
    "\n",
    "    print('train head')\n",
    "    print(df_train_X.head())\n",
    "\n",
    "    output_X_train.metadata[\"message\"] = \"Target encode all the categorical features\"\n",
    "    output_X_train.metadata[\"shape\"] = df_train_X.shape\n",
    "    output_X_train.metadata[\"format\"] = \"parquet\"        \n",
    "    output_X_train.metadata[\"type\"] = \"X_train\"\n",
    "    df_train_X.to_parquet(output_X_train.path)\n",
    "\n",
    "    output_X_test.metadata[\"message\"] = \"Target encode all the categorical features\"\n",
    "    output_X_test.metadata[\"shape\"] = df_test_X.shape\n",
    "    output_X_test.metadata[\"format\"] = \"parquet\"\n",
    "    output_X_test.metadata[\"type\"] = \"X_test\"\n",
    "    df_test_X.to_parquet(output_X_test.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6277191-ef60-458b-9072-c49ea3c84d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.9', \n",
    "           packages_to_install=['pandas', 'pyarrow', 'xgboost==1.7', 'scikit-learn'])\n",
    "def train_xgb(\n",
    "    # input_dataset\n",
    "    imported_X_train: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_y_train: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_X_test: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_y_test: Input[Dataset],\n",
    "    random_seed: int,\n",
    "    model: Output[Artifact],\n",
    "    model_path: OutputPath(str),\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    from math import sqrt\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    \n",
    "    df_train_X = pd.read_parquet(imported_X_train.path)\n",
    "    df_test_X = pd.read_parquet(imported_X_test.path)\n",
    "\n",
    "    with open(imported_y_train.path, 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(imported_y_test.path, 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    \n",
    "    xgb_reg = XGBRegressor( seed=random_seed)\n",
    "    xgb_reg.fit(df_train_X, y_train)\n",
    "    xgb_y_pred = xgb_reg.predict(df_test_X)\n",
    "    \n",
    "    scaled_rmse = sqrt(mean_squared_error(y_test, xgb_y_pred))\n",
    "    rmse = sqrt(mean_squared_error((y_test*15 + 3.464)*(y_test*15 + 3.464), (xgb_y_pred*15 + 3.464)*(xgb_y_pred*15 + 3.464)))\n",
    "    print('Scaled RMSE:', scaled_rmse)\n",
    "    print(\"RMSE of XGBoost Model on the original test data is \", rmse)\n",
    "    \n",
    "    metrics.log_metric('scaled_rmse', scaled_rmse)\n",
    "    metrics.log_metric('rmse', rmse)\n",
    "    model.metadata['type'] = 'XGBRegressor'\n",
    "\n",
    "    os.mkdir(f'{model.path}_export')\n",
    "    with open(f'{model.path}_export/model.pkl', 'wb') as f:\n",
    "        pickle.dump(xgb_reg, f)\n",
    "\n",
    "    print('the path:::::', model.path)\n",
    "    with open(model_path, 'w') as f:\n",
    "        f.write(model.path.replace('/gcs/', 'gs://').replace('/model', '/model_export'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff34d0b-e571-4a5c-88ce-45783287bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image='python:3.9', \n",
    "           packages_to_install=['pandas', 'pyarrow', 'fastai', 'torch', 'torchvision', 'torchaudio', 'scikit-learn'])\n",
    "def train_dnn(\n",
    "    # input_dataset\n",
    "    imported_X_train: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_y_train: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_X_test: Input[Dataset],\n",
    "    # input_dataset\n",
    "    imported_y_test: Input[Dataset],\n",
    "    random_seed: int,\n",
    "    model: Output[Artifact],\n",
    "    model_path: OutputPath(str),\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    from math import sqrt\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from fastai.collab import CollabDataLoaders, collab_learner\n",
    "    import random\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.random.manual_seed(random_seed)\n",
    "    torch.manual_seed(random_seed) # cpu  vars\n",
    "    torch.cuda.manual_seed_all(random_seed) # gpu \n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True  #needed\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    df_train_X = pd.read_parquet(imported_X_train.path)\n",
    "    df_test_X = pd.read_parquet(imported_X_test.path)\n",
    "\n",
    "    with open(imported_y_train.path, 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(imported_y_test.path, 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    \n",
    "    ratings_dict = {'item': list(df_train_X.Product_ID),\n",
    "                'user': list(df_train_X.User_ID),\n",
    "                'rating': list(y_train)}\n",
    "    ratings = pd.DataFrame(ratings_dict)\n",
    "\n",
    "    ratings_test_dict = {'item': list(df_test_X.Product_ID),\n",
    "                    'user': list(df_test_X.User_ID),\n",
    "                    'rating': list(y_test)}\n",
    "    ratings_test = pd.DataFrame(ratings_test_dict)\n",
    "\n",
    "    dls = CollabDataLoaders.from_df(ratings, bs=64, seed=random_seed)\n",
    "    \n",
    "    learn = collab_learner(dls, n_factors=160, use_nn=True, y_range=(0, 10))\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        learn.fit_one_cycle(5, 5e-3, wd=0.1)\n",
    "    \n",
    "    ## evaluate\n",
    "    dl = learn.dls.test_dl(ratings_test, with_labels=True)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        pred = learn.get_preds(dl=dl)\n",
    "\n",
    "    scaled_rmse = sqrt(mean_squared_error(y_test, [x.tolist()[0] for x in pred[0]]))\n",
    "    rmse = sqrt(mean_squared_error((y_test*15 + 3.464)*(y_test*15 + 3.464), \n",
    "                                   [(x.tolist()[0]*15 + 3.464)*(x.tolist()[0]*15 + 3.464) for x in pred[0]]\n",
    "                                   ))\n",
    "    print('Scaled RMSE:', scaled_rmse)\n",
    "    print(\"RMSE of XGBoost Model on the original test data is \", rmse)\n",
    "    \n",
    "    metrics.log_metric('scaled_rmse', scaled_rmse)\n",
    "    metrics.log_metric('rmse', rmse)\n",
    "    model.metadata['type'] = 'FastAI_collab_learner'\n",
    "    model.metadata['DNN_technology'] = 'Pytorch'\n",
    "    # model.metadata['containerSpec'] = {\n",
    "    #   'imageUri':\n",
    "    #       'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod'\n",
    "    # }\n",
    "    # learn.export(output_model.path)\n",
    "    # print('the path:::::', PIPELINE_ROOT, output_model.path)\n",
    "    # output_model.uri = output_model.path\n",
    "    os.mkdir(f'{model.path}_export')\n",
    "    with open(f'{model.path}_export/model.mar', 'wb') as f:\n",
    "        torch.save(learn.model, f)\n",
    "\n",
    "    print('the path:::::', model.path)\n",
    "    with open(model_path, 'w') as f:\n",
    "        f.write(model.path.replace('/gcs/', 'gs://').replace('/model', '/model_export'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce7f53d-a601-46ab-b692-fe7e65be1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"blackfriday-pipeline-v0\",\n",
    ")\n",
    "\n",
    "\n",
    "def pipeline(message: str):\n",
    "    importer = kfp.dsl.importer(\n",
    "        artifact_uri=DATA_URI,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "    dataloading_task = dataloader(\n",
    "        message=message,\n",
    "        imported_dataset=importer.output\n",
    "    )\n",
    "    transformation_task = transformation(\n",
    "        imported_dataset=dataloading_task.output\n",
    "    )\n",
    "    traintestsplit_task = traintestsplit(\n",
    "        imported_dataset=transformation_task.output,\n",
    "        random_seed=random_seed,\n",
    "        test_size=0.25\n",
    "    )\n",
    "    target_encoding_task = target_encoding(\n",
    "        imported_X_train=traintestsplit_task.outputs[\"X_train_dataset\"],\n",
    "        imported_y_train=traintestsplit_task.outputs[\"y_train_dataset\"],\n",
    "        imported_X_test=traintestsplit_task.outputs[\"X_test_dataset\"],\n",
    "    )\n",
    "    train_xgb_task = train_xgb(\n",
    "        imported_X_train=target_encoding_task.outputs[\"output_X_train\"],\n",
    "        imported_y_train=traintestsplit_task.outputs[\"y_train_dataset\"],\n",
    "        imported_X_test=target_encoding_task.outputs[\"output_X_test\"],\n",
    "        imported_y_test=traintestsplit_task.outputs[\"y_test_dataset\"],\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    train_dnn_task = train_dnn(\n",
    "        imported_X_train=traintestsplit_task.outputs[\"X_train_dataset\"],\n",
    "        imported_y_train=traintestsplit_task.outputs[\"y_train_dataset\"],\n",
    "        imported_X_test=traintestsplit_task.outputs[\"X_test_dataset\"],\n",
    "        imported_y_test=traintestsplit_task.outputs[\"y_test_dataset\"],\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    endpoint_xgb_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"blackfriday_xgb_endpoint\",\n",
    "    )\n",
    "    endpoint_dnn_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=\"blackfriday_dnn_endpoint\",\n",
    "    )\n",
    "    import_unmanaged_xgb_model_task = kfp.dsl.importer(\n",
    "        artifact_uri=train_xgb_task.outputs[\"model_path\"],\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-7:latest\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    xgb_upload_op = ModelUploadOp(\n",
    "      project=PROJECT_ID,\n",
    "      display_name=\"blackfriday_xgb\",\n",
    "      unmanaged_container_model=import_unmanaged_xgb_model_task.outputs[\"artifact\"]\n",
    "    )\n",
    "    import_unmanaged_dnn_model_task = kfp.dsl.importer(\n",
    "        artifact_uri=train_dnn_task.outputs[\"model_path\"],\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.2-0:latest\",\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    dnn_upload_op = ModelUploadOp(\n",
    "      project=PROJECT_ID,\n",
    "      display_name=\"blackfriday_dnn\",\n",
    "      unmanaged_container_model=import_unmanaged_dnn_model_task.outputs[\"artifact\"]\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        model=xgb_upload_op.outputs[\"model\"],\n",
    "        endpoint=endpoint_xgb_op.outputs[\"endpoint\"],\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        service_account=\"397218668039-compute@developer.gserviceaccount.com\"\n",
    "    )\n",
    "    ModelDeployOp(\n",
    "        model=dnn_upload_op.outputs[\"model\"],\n",
    "        endpoint=endpoint_dnn_op.outputs[\"endpoint\"],\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        service_account=\"397218668039-compute@developer.gserviceaccount.com\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043ac25e-6dfe-4a4c-8e20-1df14749ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"blackfriday_pipeline_20231101.yaml\"\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adeacdb7-4a4c-42a0-8f9b-f93bd68f632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/blackfriday-pipeline-v0-20231101052843?project=397218668039\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/397218668039/locations/us-central1/pipelineJobs/blackfriday-pipeline-v0-20231101052843\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"BlackFriday_pipeline_20231101\"\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    enable_caching=False,\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"blackfriday_pipeline_20231101.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"message\": \"BlackFriday Sales Prediction Case Study\"},\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe07a6e-0891-489e-b27b-fd57aacb2a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
